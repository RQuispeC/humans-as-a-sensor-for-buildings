{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate How Additional Users Improve Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Preperation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualisations\n",
    "from sklearn.tree import export_graphviz # Note that you need to brew install graphviz on your local machine\n",
    "import pydot \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# User Defined Functions\n",
    "import cozie_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Data Folder to Path\n",
    "data_path = os.path.abspath(os.path.join(os.path.dirname( \"__file__\" ), '..', 'data-processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cresh01', 'cresh02', 'cresh03', 'cresh04', 'cresh05', 'cresh06', 'cresh07', 'cresh08', 'cresh09', 'cresh10', 'cresh11', 'cresh12', 'cresh13', 'cresh14', 'cresh15', 'cresh16', 'cresh17', 'cresh18', 'cresh19', 'cresh20', 'cresh21', 'cresh22', 'cresh23', 'cresh24', 'cresh25', 'cresh26', 'cresh27', 'cresh28', 'cresh29', 'cresh30']\n"
     ]
    }
   ],
   "source": [
    "# The following participants took part in the experiment:\n",
    "participant_ids = ['cresh' + str(id).zfill(2) for id in range(1,31)]\n",
    "print(participant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert my dataframe into a numpy array\n",
    "def create_training_data(dataframe, preference, drop_features):\n",
    "    clean_dataframe = dataframe.copy(deep=True)\n",
    "    \n",
    "    labels = np.array(clean_dataframe[preference + '_cozie'])\n",
    "    #print(clean_dataframe.columns.values)\n",
    "    features_df = clean_dataframe.drop(drop_features, axis=1)\n",
    "    #print(features_df.columns)\n",
    "    feature_set = np.array(features_df)\n",
    "    \n",
    "    return (feature_set, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Everything within a for loop \n",
    "- read data\n",
    "- convert to features and labels\n",
    "- run the rf model\n",
    "- evalaute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape (18, 16)\n",
      "train shape (32, 16)\n",
      "Training Features Shape: (32, 7)\n",
      "Training Labels Shape: (32,)\n",
      "Summary of Thermal Prediction\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[[18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       1.00      1.00      1.00        18\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "train shape (67, 16)\n",
      "Training Features Shape: (67, 7)\n",
      "Training Labels Shape: (67,)\n",
      "Summary of Thermal Prediction\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "[[ 0  0]\n",
      " [18  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.00      0.00      0.00         0\n",
      "        11.0       0.00      0.00      0.00        18\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        18\n",
      "   macro avg       0.00      0.00      0.00        18\n",
      "weighted avg       0.00      0.00      0.00        18\n",
      "\n",
      "train shape (94, 16)\n",
      "Training Features Shape: (94, 7)\n",
      "Training Labels Shape: (94,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.7777777777777778\n",
      "0.7777777777777778\n",
      "0.2916666666666667\n",
      "[[ 0  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         9.0       0.00      0.00      0.00         1\n",
      "        10.0       0.78      1.00      0.88        14\n",
      "        11.0       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        18\n",
      "   macro avg       0.26      0.33      0.29        18\n",
      "weighted avg       0.60      0.78      0.68        18\n",
      "\n",
      "train shape (122, 16)\n",
      "Training Features Shape: (122, 7)\n",
      "Training Labels Shape: (122,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.9444444444444444\n",
      "0.9444444444444444\n",
      "0.4857142857142857\n",
      "[[ 0  1]\n",
      " [ 0 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         9.0       0.00      0.00      0.00         1\n",
      "        10.0       0.94      1.00      0.97        17\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        18\n",
      "   macro avg       0.47      0.50      0.49        18\n",
      "weighted avg       0.89      0.94      0.92        18\n",
      "\n",
      "train shape (153, 16)\n",
      "Training Features Shape: (153, 7)\n",
      "Training Labels Shape: (153,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[[18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       1.00      1.00      1.00        18\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "train shape (226, 16)\n",
      "Training Features Shape: (226, 7)\n",
      "Training Labels Shape: (226,)\n",
      "Summary of Thermal Prediction\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[[18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       1.00      1.00      1.00        18\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "train shape (254, 16)\n",
      "Training Features Shape: (254, 7)\n",
      "Training Labels Shape: (254,)\n",
      "Summary of Thermal Prediction\n",
      "0.9444444444444444\n",
      "0.9444444444444444\n",
      "0.4857142857142857\n",
      "[[17  0]\n",
      " [ 1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.94      1.00      0.97        17\n",
      "        11.0       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        18\n",
      "   macro avg       0.47      0.50      0.49        18\n",
      "weighted avg       0.89      0.94      0.92        18\n",
      "\n",
      "train shape (278, 16)\n",
      "Training Features Shape: (278, 7)\n",
      "Training Labels Shape: (278,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[[18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       1.00      1.00      1.00        18\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "train shape (309, 16)\n",
      "Training Features Shape: (309, 7)\n",
      "Training Labels Shape: (309,)\n",
      "Summary of Thermal Prediction\n",
      "0.9444444444444444\n",
      "0.9444444444444444\n",
      "0.4857142857142857\n",
      "[[17  0]\n",
      " [ 1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.94      1.00      0.97        17\n",
      "        11.0       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        18\n",
      "   macro avg       0.47      0.50      0.49        18\n",
      "weighted avg       0.89      0.94      0.92        18\n",
      "\n",
      "train shape (337, 16)\n",
      "Training Features Shape: (337, 7)\n",
      "Training Labels Shape: (337,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (362, 16)\n",
      "Training Features Shape: (362, 7)\n",
      "Training Labels Shape: (362,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (392, 16)\n",
      "Training Features Shape: (392, 7)\n",
      "Training Labels Shape: (392,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (423, 16)\n",
      "Training Features Shape: (423, 7)\n",
      "Training Labels Shape: (423,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (464, 16)\n",
      "Training Features Shape: (464, 7)\n",
      "Training Labels Shape: (464,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (486, 16)\n",
      "Training Features Shape: (486, 7)\n",
      "Training Labels Shape: (486,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (528, 16)\n",
      "Training Features Shape: (528, 7)\n",
      "Training Labels Shape: (528,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (553, 16)\n",
      "Training Features Shape: (553, 7)\n",
      "Training Labels Shape: (553,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (590, 16)\n",
      "Training Features Shape: (590, 7)\n",
      "Training Labels Shape: (590,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (615, 16)\n",
      "Training Features Shape: (615, 7)\n",
      "Training Labels Shape: (615,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (642, 16)\n",
      "Training Features Shape: (642, 7)\n",
      "Training Labels Shape: (642,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (673, 16)\n",
      "Training Features Shape: (673, 7)\n",
      "Training Labels Shape: (673,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.45454545454545453\n",
      "[[15  0]\n",
      " [ 3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.83      1.00      0.91        15\n",
      "        11.0       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        18\n",
      "   macro avg       0.42      0.50      0.45        18\n",
      "weighted avg       0.69      0.83      0.76        18\n",
      "\n",
      "train shape (696, 16)\n",
      "Training Features Shape: (696, 7)\n",
      "Training Labels Shape: (696,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (723, 16)\n",
      "Training Features Shape: (723, 7)\n",
      "Training Labels Shape: (723,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.45454545454545453\n",
      "[[15  0]\n",
      " [ 3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.83      1.00      0.91        15\n",
      "        11.0       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        18\n",
      "   macro avg       0.42      0.50      0.45        18\n",
      "weighted avg       0.69      0.83      0.76        18\n",
      "\n",
      "train shape (751, 16)\n",
      "Training Features Shape: (751, 7)\n",
      "Training Labels Shape: (751,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.45454545454545453\n",
      "[[15  0]\n",
      " [ 3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.83      1.00      0.91        15\n",
      "        11.0       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        18\n",
      "   macro avg       0.42      0.50      0.45        18\n",
      "weighted avg       0.69      0.83      0.76        18\n",
      "\n",
      "train shape (774, 16)\n",
      "Training Features Shape: (774, 7)\n",
      "Training Labels Shape: (774,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8333333333333334\n",
      "0.8333333333333334\n",
      "0.45454545454545453\n",
      "[[15  0]\n",
      " [ 3  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.83      1.00      0.91        15\n",
      "        11.0       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        18\n",
      "   macro avg       0.42      0.50      0.45        18\n",
      "weighted avg       0.69      0.83      0.76        18\n",
      "\n",
      "train shape (800, 16)\n",
      "Training Features Shape: (800, 7)\n",
      "Training Labels Shape: (800,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (818, 16)\n",
      "Training Features Shape: (818, 7)\n",
      "Training Labels Shape: (818,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (836, 16)\n",
      "Training Features Shape: (836, 7)\n",
      "Training Labels Shape: (836,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (866, 16)\n",
      "Training Features Shape: (866, 7)\n",
      "Training Labels Shape: (866,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n",
      "train shape (885, 16)\n",
      "Training Features Shape: (885, 7)\n",
      "Training Labels Shape: (885,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Thermal Prediction\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "0.47058823529411764\n",
      "[[16  0]\n",
      " [ 2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        10.0       0.89      1.00      0.94        16\n",
      "        11.0       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.44      0.50      0.47        18\n",
      "weighted avg       0.79      0.89      0.84        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Feature Set the \n",
    "feature_set = 'fs5'\n",
    "\n",
    "# Define the participant test set to use. TODO: Cycle through all participants and average\n",
    "participant = 'cresh03'\n",
    "\n",
    "# Read Test Set Data\n",
    "test_set_df = pd.read_csv(os.path.join(data_path, \n",
    "            '2019-11-15_' + feature_set + '_val_' + participant + '.csv'))\n",
    "\n",
    "# labels to drop. TODO: Eventually have this dynamic\n",
    "# TODO Don't drop rooms\n",
    "thermal_drop_features = ['light_cozie', 'noise_cozie', 'user_id', 'thermal_cozie',\n",
    "                        'prefer_dimmer', 'prefer_brighter', 'prefer_quieter', 'prefer_louder', 'room']\n",
    "\n",
    "# Convert test set data to labels\n",
    "test_features, test_labels = create_training_data(test_set_df, 'thermal', thermal_drop_features)\n",
    "\n",
    "\n",
    "\n",
    "print(\"test shape\", test_set_df.shape)\n",
    "\n",
    "\n",
    "# Empty dataframe to append results\n",
    "train_set_df = pd.DataFrame()\n",
    "\n",
    "for participant in participant_ids:\n",
    "    \n",
    "    new_train_set_df = pd.read_csv(os.path.join(data_path, \n",
    "            '2019-11-15_' + feature_set + '_train_' + participant + '.csv'))\n",
    "    \n",
    "    train_set_df = pd.concat([train_set_df, new_train_set_df])\n",
    "    \n",
    "    print('train shape', train_set_df.shape)\n",
    "\n",
    "    train_features, train_labels = create_training_data(train_set_df, 'thermal', thermal_drop_features)\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    \n",
    "\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    thermal_rf = RandomForestClassifier(n_estimators = 1000, random_state = 42 )\n",
    "    # Train the model on training data\n",
    "    thermal_rf.fit(train_features, train_labels);\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = thermal_rf.predict(test_features)\n",
    "    \n",
    "    print(\"Summary of Thermal Prediction\")\n",
    "    print(metrics.accuracy_score(predictions, test_labels))\n",
    "    print(metrics.f1_score(predictions, test_labels, average=\"micro\")) # Micro averages all the results\n",
    "    print(metrics.f1_score(predictions, test_labels, average=\"macro\")) #Macro treats each class equally \n",
    "    print(metrics.confusion_matrix(predictions, test_labels))\n",
    "    print(metrics.classification_report(predictions,test_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Rooms and user Id to binary inputs (Hot Encoding)\n",
    "## Modified in V3, and V5\n",
    "\n",
    " ## TODO: Get the hot encoding working from the test data\n",
    "#train_set_df = pd.get_dummies(train_set_df, columns=['room'])\n",
    "#test_set_df = pd.get_dummies(test_set_df, columns=['room'])\n",
    "#print(list(train_set_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into numpy arrays for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (885, 8)\n",
      "Training Labels Shape: (885,)\n",
      "Testing Features Shape: (21, 8)\n",
      "Testing Labels Shape: (21,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Usign RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
